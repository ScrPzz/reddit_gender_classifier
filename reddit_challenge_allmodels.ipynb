{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit user's gender classificator \n",
    "\n",
    "\n",
    "Nome: Alessandro Togni\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I don't think we'd get nearly as much fanficti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Thanks. I made it up, that's how I got over my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Are you sure you aren't confusing Cyclops (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>dont do this to me bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>That's what we do when we can't find a mate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sahil17</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Damn I love this question. Here's what I think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spyfer</td>\n",
       "      <td>NetflixBestOf</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Sorry about that one post btw lol. I hope it d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tehftw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Ad 1 &amp;amp; 2. Right. I was mistaken.\\n\\nAd 3. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timsstuff</td>\n",
       "      <td>movies</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I thought it was Colm Meaney whenever he's in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bullseye4hire</td>\n",
       "      <td>hockey</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>FAck. Looks like 4 mins.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit   created_utc  \\\n",
       "0    Shamus_Aran       mylittlepony  1.388534e+09   \n",
       "1       Riddance                sex  1.388534e+09   \n",
       "2  Secret_Wizard       DragonsDogma  1.388534e+09   \n",
       "3   Penultimatum  malefashionadvice  1.388534e+09   \n",
       "4      7-SE7EN-7      todayilearned  1.388534e+09   \n",
       "5        Sahil17       Christianity  1.388534e+09   \n",
       "6         spyfer      NetflixBestOf  1.388534e+09   \n",
       "7         tehftw          AskReddit  1.388534e+09   \n",
       "8      timsstuff             movies  1.388534e+09   \n",
       "9  Bullseye4hire             hockey  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I don't think we'd get nearly as much fanficti...  \n",
       "1  Thanks. I made it up, that's how I got over my...  \n",
       "2  Are you sure you aren't confusing Cyclops (the...  \n",
       "3                             dont do this to me bro  \n",
       "4        That's what we do when we can't find a mate  \n",
       "5  Damn I love this question. Here's what I think...  \n",
       "6  Sorry about that one post btw lol. I hope it d...  \n",
       "7  Ad 1 &amp; 2. Right. I was mistaken.\\n\\nAd 3. ...  \n",
       "8  I thought it was Colm Meaney whenever he's in ...  \n",
       "9                           FAck. Looks like 4 mins.  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/home/ale/Scrivania/reddit_chal/train_data.csv\", encoding=\"utf8\")\n",
    "clean_train_data=train_data\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_train_data['word_length'] = clean_train_data['body'].map(lambda x: len(x.strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296042"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Cleaning text\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(\" \\d+\", \" \", x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(\"\\d+\", \" \", x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('\\[.*?\\]', '', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '',  x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('<.*?>+', '',  x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('\\w*\\d\\w*', '', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('^a-zA-Z0-9 -', '', x))\n",
    "clean_train_data.body = clean_train_data.body.str.replace(\"(<br/>)\", \"\")\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(<a).*(>).*(</a>)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(&amp)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(&gt)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(&lt)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(\\xa0)', '')  \n",
    "\n",
    "\n",
    "\n",
    "#Making all text lowercase and getting rid of the date column\n",
    "clean_train_data['body']=clean_train_data['body'].str.lower()\n",
    "clean_train_data=clean_train_data.drop(['created_utc'], axis=1)\n",
    "\n",
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>wed nearly fanfiction pictures shipping banban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>thanks got heart break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>sure arent confusing cyclops easiest boss mons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>mate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sahil17</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>damn love question heres church fathers earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spyfer</td>\n",
       "      <td>NetflixBestOf</td>\n",
       "      <td>sorry post btw lol hope didnt cause guys troub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tehftw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>right mistaken firearms access weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timsstuff</td>\n",
       "      <td>movies</td>\n",
       "      <td>thought colm meaney hes role colm meaney john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bullseye4hire</td>\n",
       "      <td>hockey</td>\n",
       "      <td>fack looks mins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit  \\\n",
       "0    Shamus_Aran       mylittlepony   \n",
       "1       Riddance                sex   \n",
       "2  Secret_Wizard       DragonsDogma   \n",
       "3   Penultimatum  malefashionadvice   \n",
       "4      7-SE7EN-7      todayilearned   \n",
       "5        Sahil17       Christianity   \n",
       "6         spyfer      NetflixBestOf   \n",
       "7         tehftw          AskReddit   \n",
       "8      timsstuff             movies   \n",
       "9  Bullseye4hire             hockey   \n",
       "\n",
       "                                                body  \n",
       "0  wed nearly fanfiction pictures shipping banban...  \n",
       "1                             thanks got heart break  \n",
       "2  sure arent confusing cyclops easiest boss mons...  \n",
       "3                                                bro  \n",
       "4                                               mate  \n",
       "5  damn love question heres church fathers earlie...  \n",
       "6  sorry post btw lol hope didnt cause guys troub...  \n",
       "7             right mistaken firearms access weapons  \n",
       "8  thought colm meaney hes role colm meaney john ...  \n",
       "9                                    fack looks mins  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords taken both from the ntlk that gensim corpus (the appended ones)\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "stopwords.extend(['know', 'youre', 'want','thats','ad', 'amp', 'id', 'dont','like', 'people','im', 'dont', 'one', 'think', 'would', 'during', 'among', 'thereafter', 'only', 'hers', 'in', 'none', 'with', 'un', 'put', 'hence', 'each', 'would', 'have', 'to', 'itself', 'that', 'seeming', 'hereupon', 'someone', 'eight', 'she', 'forty', 'much', 'throughout', 'less', 'was', 'interest', 'elsewhere', 'already', 'whatever', 'or', 'seem', 'fire', 'however', 'keep', 'detail', 'both', 'yourselves', 'indeed', 'enough', 'too', 'us', 'wherein', 'himself', 'behind', 'everything', 'part', 'made', 'thereupon', 'for', 'nor', 'before', 'front', 'sincere', 'really', 'than', 'alone', 'doing', 'amongst', 'across', 'him', 'another', 'some', 'whoever', 'four', 'other', 'latterly', 'off', 'sometime', 'above', 'often', 'herein', 'am', 'whereby', 'although', 'who', 'should', 'amount', 'anyway', 'else', 'upon', 'this', 'when', 'we', 'few', 'anywhere', 'will', 'though', 'being', 'fill', 'used', 'full', 'thru', 'call', 'whereafter', 'various', 'has', 'same', 'former', 'whereas', 'what', 'had', 'mostly', 'onto', 'go', 'could', 'yourself', 'meanwhile', 'beyond', 'beside', 'ours', 'side', 'our', 'five', 'nobody', 'herself', 'is', 'ever', 'they', 'here', 'eleven', 'fifty', 'therefore', 'nothing', 'not', 'mill', 'without', 'whence', 'get', 'whither', 'then', 'no', 'own', 'many', 'anything', 'etc', 'make', 'from', 'against', 'ltd', 'next', 'afterwards', 'unless', 'while', 'thin', 'beforehand', 'by', 'amoungst', 'you', 'third', 'as', 'those', 'done', 'becoming', 'say', 'either', 'doesn', 'twenty', 'his', 'yet', 'latter', 'somehow', 'are', 'these', 'mine', 'under', 'take', 'whose', 'others', 'over', 'perhaps', 'thence', 'does', 'where', 'two', 'always', 'your', 'wherever', 'became', 'which', 'about', 'but', 'towards', 'still', 'rather', 'quite', 'whether', 'somewhere', 'might', 'do', 'bottom', 'until', 'km', 'yours', 'serious', 'find', 'please', 'hasnt', 'otherwise', 'six', 'toward', 'sometimes', 'of', 'fifteen', 'eg', 'just', 'a', 'me', 'describe', 'why', 'an', 'and', 'may', 'within', 'kg', 'con', 're', 'nevertheless', 'through', 'very', 'anyhow', 'down', 'nowhere', 'now', 'it', 'cant', 'de', 'move', 'hereby', 'how', 'found', 'whom', 'were', 'together', 'again', 'moreover', 'first', 'never', 'below', 'between', 'computer', 'ten', 'into', 'see', 'everywhere', 'there', 'neither', 'every', 'couldnt', 'up', 'several', 'the', 'i', 'becomes', 'don', 'ie', 'been', 'whereupon', 'seemed', 'most', 'noone', 'whole', 'must', 'cannot', 'per', 'my', 'thereby', 'so', 'he', 'name', 'co', 'its', 'everyone', 'if', 'become', 'thick', 'thus', 'regarding', 'didn', 'give', 'all', 'show', 'any', 'using', 'on', 'further', 'around', 'back', 'least', 'since', 'anyone', 'once', 'can', 'bill', 'hereafter', 'be', 'seems', 'their', 'myself', 'nine', 'also', 'system', 'at', 'more', 'out', 'twelve', 'therein', 'almost', 'except', 'last', 'did', 'something', 'besides', 'via', 'whenever', 'formerly', 'cry', 'one', 'hundred', 'sixty', 'after', 'well', 'them', 'namely', 'empty', 'three', 'even', 'along', 'because', 'ourselves', 'such', 'top', 'due', 'inc', 'themselves'])\n",
    "\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: ' '.join([words for words in x.split() if words not in stopwords]))\n",
    "clean_train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer and stemmer do not seems to make a real difference\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "#from nltk import punkt\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "  \n",
    "#lemmatizer = WordNetLemmatizer() \n",
    "#clean_train_data['body']=clean_train_data['body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "\n",
    "\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#stemmer=LancasterStemmer()\n",
    "\n",
    "#clean_train_data['body']=clean_train_data['body'].apply(lambda x: ' '.join([stemmer.stem(w) for w in nltk.word_tokenize(x)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>wed nearly fanfiction pictures shipping banban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>thanks got heart break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>sure arent confusing cyclops easiest boss mons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>mate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sahil17</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>damn love question heres church fathers earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spyfer</td>\n",
       "      <td>NetflixBestOf</td>\n",
       "      <td>sorry post btw lol hope didnt cause guys troub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tehftw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>right mistaken firearms access weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timsstuff</td>\n",
       "      <td>movies</td>\n",
       "      <td>thought colm meaney hes role colm meaney john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bullseye4hire</td>\n",
       "      <td>hockey</td>\n",
       "      <td>fack looks mins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit  \\\n",
       "0    Shamus_Aran       mylittlepony   \n",
       "1       Riddance                sex   \n",
       "2  Secret_Wizard       DragonsDogma   \n",
       "3   Penultimatum  malefashionadvice   \n",
       "4      7-SE7EN-7      todayilearned   \n",
       "5        Sahil17       Christianity   \n",
       "6         spyfer      NetflixBestOf   \n",
       "7         tehftw          AskReddit   \n",
       "8      timsstuff             movies   \n",
       "9  Bullseye4hire             hockey   \n",
       "\n",
       "                                                body  \n",
       "0  wed nearly fanfiction pictures shipping banban...  \n",
       "1                             thanks got heart break  \n",
       "2  sure arent confusing cyclops easiest boss mons...  \n",
       "3                                                bro  \n",
       "4                                               mate  \n",
       "5  damn love question heres church fathers earlie...  \n",
       "6  sorry post btw lol hope didnt cause guys troub...  \n",
       "7             right mistaken firearms access weapons  \n",
       "8  thought colm meaney hes role colm meaney john ...  \n",
       "9                                    fack looks mins  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"/home/ale/Scrivania/reddit_chal/train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mylittlepony            0\n",
      "sex                     1\n",
      "DragonsDogma            2\n",
      "malefashionadvice       3\n",
      "todayilearned           4\n",
      "                     ... \n",
      "palegirls            3463\n",
      "onions               3464\n",
      "mumfordandsons       3465\n",
      "infertility          3466\n",
      "HangoutFest          3467\n",
      "Length: 3468, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Extract the subreddit list and mapping it to integers\n",
    "subreddits = clean_train_data.subreddit.unique()\n",
    "subreddits_map = pd.Series(index=subreddits, data=arange(subreddits.shape[0]))\n",
    "\n",
    "print(subreddits_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(group):\n",
    "    group_subreddits = group['subreddit'].values \n",
    "    #print(group_subreddits.type)\n",
    "    idxs = subreddits_map[group_subreddits].values \n",
    "    v = sparse.dok_matrix((1, subreddits.shape[0]))\n",
    "  \n",
    "    \n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] = 1\n",
    "    \n",
    " \n",
    "    return v.tocsr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "   \n",
    "    features_dict[author] = extract_features(group)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.vstack([features_dict[author] for author in target.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = target.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(group):\n",
    "    group_text = group['body'].values\n",
    "    return \" \".join(group_text)\n",
    "\n",
    "def extract_subrteddit_text(group):\n",
    "    group_text = group['subreddit'].values\n",
    "    return \" \".join(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "    text_dict[author] = extract_text(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_text = [text_dict[author] for author in target.author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_dict ={}\n",
    "author_dict = {}\n",
    "\n",
    "for subreddit, group in clean_train_data.groupby('subreddit'):\n",
    "    subreddit_dict[subreddit] = extract_text(group)\n",
    "\n",
    "    \n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "    author_dict[author] = extract_text(group)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200882 176730\n"
     ]
    }
   ],
   "source": [
    "#Creating list of words taken from the corpus of top subreddit for men and women\n",
    "#sources: https://www.techlazy.com/best-subreddits-for-men/\n",
    "#http://redditlist.com/\n",
    "\n",
    "\n",
    "men_list=[]\n",
    "men_list.extend(subreddit_dict['malefashionadvice'].split(' '))\n",
    "men_list.extend(subreddit_dict['MensRights'].split(' '))\n",
    "men_list.extend(subreddit_dict['malehairadvice'].split(' '))\n",
    "men_list.extend(subreddit_dict['beards'].split(' '))\n",
    "men_list.extend(subreddit_dict['AskMen'].split(' '))\n",
    "men_list.extend(subreddit_dict['AskMenOver30'].split(' '))\n",
    "\n",
    "\n",
    "women_list=[]\n",
    "women_list.extend(subreddit_dict['AskWomen'].split(' '))\n",
    "women_list.extend(subreddit_dict['Feminism'].split(' '))\n",
    "women_list.extend(subreddit_dict['TheGirlSurvivalGuide'].split(' '))\n",
    "women_list.extend(subreddit_dict['GirlGamers'].split(' '))\n",
    "women_list.extend(subreddit_dict['TwoXChromosomes'].split(' '))\n",
    "women_list.extend(subreddit_dict['AskWomenOver30'].split(' '))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(women_list), len(men_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to sort the wordlists by number of appearences and eliminate duplicates\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def sort_unique(lst, n):\n",
    "    result = (item for items, c in Counter(lst).most_common() for item in [items] * c)\n",
    "    unique_list = list(dict.fromkeys(result))\n",
    "    if len (lst)> n:\n",
    "            unique_list = unique_list[:n]\n",
    "    else: \n",
    "            unique_list=unique_list[:len(lst)]\n",
    "    return unique_list\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time              0\n",
      "women             1\n",
      "good              2\n",
      "way               3\n",
      "ive               4\n",
      "               ... \n",
      "prospects      4995\n",
      "carrying       4996\n",
      "furthermore    4997\n",
      "damages        4998\n",
      "approve        4999\n",
      "Length: 5000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "men_sorted_unique=  np.array(sort_unique(subreddit_dict['AskMen'].split(' '), 5000))\n",
    "women_sorted_unique= np.array(sort_unique(subreddit_dict['AskWomen'].split(' '), 5000))\n",
    "\n",
    "M_subreddit_gender_map = pd.Series(index=men_sorted_unique, data=arange(men_sorted_unique.shape[0]))\n",
    "F_subreddit_gender_map = pd.Series(index=women_sorted_unique, data=arange(women_sorted_unique.shape[0]))\n",
    "\n",
    "\n",
    "print(M_subreddit_gender_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create sparse matrices relatives to the m/f features\n",
    "\n",
    "def extract_Mgender_features(group):\n",
    "\n",
    "   \n",
    "    group_text = group['body'].values\n",
    "                        \n",
    "    idxs = M_subreddit_gender_map[group_text].values\n",
    "\n",
    "    \n",
    "    #print(idxs)\n",
    "    v = sparse.dok_matrix((1, men_sorted_unique.shape[0]))\n",
    "    #print(v)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] =1\n",
    "        \n",
    "    \n",
    "    \n",
    "    return v.tocsr()\n",
    "\n",
    "\n",
    "def extract_Fgender_features(group):\n",
    "\n",
    "   \n",
    "    group_text = group['body'].values\n",
    "                        \n",
    "    idxs = F_subreddit_gender_map[group_text].values\n",
    "    #print(idxs)\n",
    "    \n",
    "    #print(idxs)\n",
    "    v = sparse.dok_matrix((1, women_sorted_unique.shape[0]))\n",
    "    #print(v)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] +=1\n",
    "        \n",
    "    \n",
    "    \n",
    "    return v.tocsr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the m/dictionaries\n",
    "\n",
    "M_features_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "   \n",
    "    M_features_dict[author] = extract_Mgender_features(group)\n",
    "    \n",
    "    \n",
    "F_features_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "   \n",
    "    F_features_dict[author] = extract_Fgender_features(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sparse matrices for m/f contributiion\n",
    "Xf = sparse.vstack([F_features_dict[author] for author in target.author])\n",
    "Xm = sparse.vstack([M_features_dict[author] for author in target.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x23468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 100360 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stacking all the sparse matrices\n",
    "X=sparse.hstack([X, Xm, Xf]).tocsr()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score ,f1_score,roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form now on i'll try some different classifiers and choose the most performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#\n",
    "\n",
    "clf_lr = LogisticRegression(C=5, max_iter=100)\n",
    "clf_lr.fit(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       366\n",
      "           1       0.83      0.57      0.68       134\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.84      0.77      0.79       500\n",
      "weighted avg       0.85      0.85      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[350,  16],\n",
       "       [ 57,  77]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713576158940397"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
    "kfold_lr = KFold(n_splits=10)\n",
    "result_lr = cross_val_predict(estimator=clf_lr,X=X,y=y,cv=kfold_lr)\n",
    "\n",
    "\n",
    "f1_lr=f1_score(y, result_lr)\n",
    "\n",
    "f1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lr = accuracy_score(y, result_lr)\n",
    "acc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_lr = roc_auc_score(y, y_score=result_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Model\n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(kernel='linear', probability=True)\n",
    "clf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc = clf_svc.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       366\n",
      "           1       0.84      0.57      0.68       134\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.85      0.76      0.79       500\n",
      "weighted avg       0.85      0.85      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[351,  15],\n",
       "       [ 58,  76]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_svc = KFold(n_splits=10)\n",
    "result_svc = cross_val_predict(estimator=clf_svc,X=X,y=y,cv=kfold_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6564315352697095"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_svc=f1_score(y, result_svc)\n",
    "f1_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8344"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_svc = accuracy_score(y, result_svc)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7562039625200931"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score_svc = roc_auc_score(y, y_score=result_svc)\n",
    "roc_auc_score_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.839 0.848 0.834 0.831 0.84 ]\n",
      "0.8384\n"
     ]
    }
   ],
   "source": [
    "cross_val_svc = cross_val_score(SVC(kernel='linear', probability=True),X,y, cv=5, scoring='accuracy')\n",
    "print (cross_val_svc)\n",
    "print (cross_val_svc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choice: {'max_depth': None, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf_1 = RandomForestClassifier()\n",
    "Cs_1 = [10, 500]\n",
    "Cs_2 = [None, 50, 400]\n",
    "grid_1 = GridSearchCV(clf_1, param_grid={'n_estimators': Cs_1,'max_depth': Cs_2}, cv=5)\n",
    "grid_1.fit(X_train, y_train)\n",
    "print (\"best parameter choice:\", grid_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=grid_1.best_params_['n_estimators'], max_depth=grid_1.best_params_['max_depth'])\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       366\n",
      "           1       0.73      0.51      0.61       134\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.79      0.72      0.74       500\n",
      "weighted avg       0.81      0.82      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[341,  25],\n",
       "       [ 65,  69]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6231028667790894"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_rf = KFold(n_splits=5, random_state=42)\n",
    "result_rf = cross_val_predict(estimator=clf_rf,X=X,y=y,cv=kfold_rf)\n",
    "f1_rf=f1_score(y, result_rf)\n",
    "f1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_rf = accuracy_score(y, result_rf)\n",
    "acc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7350131436313538"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score_rf = roc_auc_score(y, y_score=result_rf)\n",
    "roc_auc_score_rf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiNB=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiNB.fit(X_train,y_train)\n",
    "y_pred_MultiNB = MultiNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred_MultiNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91       366\n",
      "           1       0.89      0.55      0.68       134\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.87      0.76      0.80       500\n",
      "weighted avg       0.87      0.86      0.85       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred_MultiNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[357,   9],\n",
       "       [ 60,  74]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_MultiNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_NB = KFold(n_splits=10)\n",
    "result_NB = cross_val_predict(estimator=MultiNB,X=X,y=y,cv=kfold_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8526"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_NB = accuracy_score(y, result_NB)\n",
    "acc_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532423562986998"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score_NB = roc_auc_score(y, y_score=result_NB)\n",
    "roc_auc_score_NB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiNB is the best classifier for this problem\n"
     ]
    }
   ],
   "source": [
    "classifiers = ['Logistic Regression', 'Random Forest Classifier', 'SVC', 'MultiNB']\n",
    "acc = np.array([acc_lr, acc_rf, acc_svc, acc_NB])\n",
    "max_acc = np.argmax(acc)\n",
    "rocauc = np.array([roc_auc_score_lr, roc_auc_score_rf, roc_auc_score_svc, roc_auc_score_NB])\n",
    "max_rocauc = np.argmax(rocauc)\n",
    "print (classifiers[max_acc] + ' is the best classifier for this problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ejchristian86</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I hadn't ever heard of them before joining thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZenDragon</td>\n",
       "      <td>gaming</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>At 7680 by 4320 with 64x AA, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>savoytruffle</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>bite me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hentercenter</td>\n",
       "      <td>stlouisblues</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Damn that was a good penalty :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rick-o-suave</td>\n",
       "      <td>army</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I swore into DEP on 6-OCT and I left 5-NOV und...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author        subreddit   created_utc  \\\n",
       "0  ejchristian86  TwoXChromosomes  1.388534e+09   \n",
       "1      ZenDragon           gaming  1.388534e+09   \n",
       "2   savoytruffle        AskReddit  1.388534e+09   \n",
       "3   hentercenter     stlouisblues  1.388534e+09   \n",
       "4   rick-o-suave             army  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I hadn't ever heard of them before joining thi...  \n",
       "1                At 7680 by 4320 with 64x AA, right?  \n",
       "2                                            bite me  \n",
       "3                    Damn that was a good penalty :(  \n",
       "4  I swore into DEP on 6-OCT and I left 5-NOV und...  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/home/ale/Scrivania/reddit_chal/test_data.csv\", encoding=\"utf8\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_test_data=test_data\n",
    "clean_test_data=clean_test_data.drop(['created_utc'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_data.isna().values.any()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1063323</th>\n",
       "      <td>SketchingShibe</td>\n",
       "      <td>dogecoin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author subreddit body\n",
       "1063323  SketchingShibe  dogecoin  NaN"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_data[pd.isnull(clean_test_data).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_test_data=clean_test_data.drop(clean_test_data.index[1063323])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_data.isna().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub('\\s+', ' ', str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub(r\"http\\S+\", \"\", str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub(\" \\d+\", \" \", str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub(\"\\d+\", \" \", str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub('\\[.*?\\]', '', str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '',  str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub('<.*?>+', '',  str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub('\\w*\\d\\w*', '', str(x)))\n",
    "clean_test_data['body']=test_data['body'].apply(lambda x: re.sub('^a-zA-Z0-9 -', '', str(x)))\n",
    "clean_test_data.body = test_data.body.str.replace(\"(<br/>)\", \"\")\n",
    "clean_test_data.body = test_data.body.str.replace('(<a).*(>).*(</a>)', '')\n",
    "clean_test_data.body = test_data.body.str.replace('(&amp)', '')\n",
    "clean_test_data.body = test_data.body.str.replace('(&gt)', '')\n",
    "clean_test_data.body = test_data.body.str.replace('(&lt)', '')\n",
    "clean_test_data.body = test_data.body.str.replace('(\\xa0)', '') \n",
    "\n",
    "\n",
    "clean_test_data['body']=clean_test_data['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict_test = {}\n",
    "\n",
    "for author, group in clean_test_data.groupby('author'):\n",
    "    features_dict_test[author] = extract_features(group)\n",
    "    \n",
    "M_features_dict_test = {}\n",
    "\n",
    "for author, group in clean_test_data.groupby('author'):\n",
    "   \n",
    "    M_features_dict_test[author] = extract_Mgender_features(group)\n",
    "    \n",
    "    \n",
    "F_features_dict_test = {}\n",
    "\n",
    "for author, group in clean_test_data.groupby('author'):\n",
    "   \n",
    "    F_features_dict_test[author] = extract_Fgender_features(group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r_test = sparse.vstack([features_dict_test[author] for author in test_data.author.unique()])\n",
    "X_f_test = sparse.vstack([M_features_dict_test[author] for author in test_data.author.unique()])\n",
    "X_m_test = sparse.vstack([F_features_dict_test[author] for author in test_data.author.unique()])\n",
    "X_test=sparse.hstack([X_r_test, X_f_test,X_m_test]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MultiNB.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ejchristian86</td>\n",
       "      <td>9.999995e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZenDragon</td>\n",
       "      <td>4.627192e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>savoytruffle</td>\n",
       "      <td>5.503937e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hentercenter</td>\n",
       "      <td>3.777475e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rick-o-suave</td>\n",
       "      <td>1.301911e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author        gender\n",
       "0  ejchristian86  9.999995e-01\n",
       "1      ZenDragon  4.627192e-10\n",
       "2   savoytruffle  5.503937e-07\n",
       "3   hentercenter  3.777475e-03\n",
       "4   rick-o-suave  1.301911e-01"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = pd.DataFrame({\"author\":test_data.author.unique(), \"gender\":y_pred})\n",
    "solution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.to_csv(\"solution.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author,gender\n",
      "ejchristian86,0.9999994543579543\n",
      "ZenDragon,4.6271917249232285e-10\n",
      "savoytruffle,5.50393723453192e-07\n",
      "hentercenter,0.0037774747813984154\n",
      "rick-o-suave,0.13019105020158062\n",
      "olivermihoff,0.00017825927037081585\n",
      "Cleriesse,0.8705007001761501\n",
      "murderer_of_death,1.0295161014144079e-09\n",
      "SpiralSoul,1.35552808709269e-09\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "head solution.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
