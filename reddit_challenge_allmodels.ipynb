{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit user's gender classificator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ale/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package stopwords to /home/ale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I don't think we'd get nearly as much fanficti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Thanks. I made it up, that's how I got over my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Are you sure you aren't confusing Cyclops (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>dont do this to me bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>That's what we do when we can't find a mate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sahil17</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Damn I love this question. Here's what I think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spyfer</td>\n",
       "      <td>NetflixBestOf</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Sorry about that one post btw lol. I hope it d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tehftw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Ad 1 &amp;amp; 2. Right. I was mistaken.\\n\\nAd 3. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timsstuff</td>\n",
       "      <td>movies</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I thought it was Colm Meaney whenever he's in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bullseye4hire</td>\n",
       "      <td>hockey</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>FAck. Looks like 4 mins.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit   created_utc  \\\n",
       "0    Shamus_Aran       mylittlepony  1.388534e+09   \n",
       "1       Riddance                sex  1.388534e+09   \n",
       "2  Secret_Wizard       DragonsDogma  1.388534e+09   \n",
       "3   Penultimatum  malefashionadvice  1.388534e+09   \n",
       "4      7-SE7EN-7      todayilearned  1.388534e+09   \n",
       "5        Sahil17       Christianity  1.388534e+09   \n",
       "6         spyfer      NetflixBestOf  1.388534e+09   \n",
       "7         tehftw          AskReddit  1.388534e+09   \n",
       "8      timsstuff             movies  1.388534e+09   \n",
       "9  Bullseye4hire             hockey  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I don't think we'd get nearly as much fanficti...  \n",
       "1  Thanks. I made it up, that's how I got over my...  \n",
       "2  Are you sure you aren't confusing Cyclops (the...  \n",
       "3                             dont do this to me bro  \n",
       "4        That's what we do when we can't find a mate  \n",
       "5  Damn I love this question. Here's what I think...  \n",
       "6  Sorry about that one post btw lol. I hope it d...  \n",
       "7  Ad 1 &amp; 2. Right. I was mistaken.\\n\\nAd 3. ...  \n",
       "8  I thought it was Colm Meaney whenever he's in ...  \n",
       "9                           FAck. Looks like 4 mins.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/home/ale/Scrivania/reddit_chal/train_data.csv\", encoding=\"utf8\")\n",
    "clean_train_data=train_data\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_train_data['word_length'] = clean_train_data['body'].map(lambda x: len(x.strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296042"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Cleaning text\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(\" \\d+\", \" \", x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub(\"\\d+\", \" \", x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('\\[.*?\\]', '', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('https?://\\S+|www\\.\\S+', '',  x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('<.*?>+', '',  x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('\\w*\\d\\w*', '', x))\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: re.sub('^a-zA-Z0-9 -', '', x))\n",
    "clean_train_data.body = clean_train_data.body.str.replace(\"(<br/>)\", \"\")\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(<a).*(>).*(</a>)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(&amp)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(&gt)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(&lt)', '')\n",
    "clean_train_data.body = clean_train_data.body.str.replace('(\\xa0)', '')  \n",
    "\n",
    "\n",
    "\n",
    "#Making all text lowercase and getting rid of the date column\n",
    "clean_train_data['body']=clean_train_data['body'].str.lower()\n",
    "clean_train_data=clean_train_data.drop(['created_utc'], axis=1)\n",
    "\n",
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>wed nearly fanfiction pictures shipping banban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>thanks got heart break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>sure arent confusing cyclops easiest boss mons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>mate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sahil17</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>damn love question heres church fathers earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spyfer</td>\n",
       "      <td>NetflixBestOf</td>\n",
       "      <td>sorry post btw lol hope didnt cause guys troub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tehftw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>right mistaken firearms access weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timsstuff</td>\n",
       "      <td>movies</td>\n",
       "      <td>thought colm meaney hes role colm meaney john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bullseye4hire</td>\n",
       "      <td>hockey</td>\n",
       "      <td>fack looks mins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit  \\\n",
       "0    Shamus_Aran       mylittlepony   \n",
       "1       Riddance                sex   \n",
       "2  Secret_Wizard       DragonsDogma   \n",
       "3   Penultimatum  malefashionadvice   \n",
       "4      7-SE7EN-7      todayilearned   \n",
       "5        Sahil17       Christianity   \n",
       "6         spyfer      NetflixBestOf   \n",
       "7         tehftw          AskReddit   \n",
       "8      timsstuff             movies   \n",
       "9  Bullseye4hire             hockey   \n",
       "\n",
       "                                                body  \n",
       "0  wed nearly fanfiction pictures shipping banban...  \n",
       "1                             thanks got heart break  \n",
       "2  sure arent confusing cyclops easiest boss mons...  \n",
       "3                                                bro  \n",
       "4                                               mate  \n",
       "5  damn love question heres church fathers earlie...  \n",
       "6  sorry post btw lol hope didnt cause guys troub...  \n",
       "7             right mistaken firearms access weapons  \n",
       "8  thought colm meaney hes role colm meaney john ...  \n",
       "9                                    fack looks mins  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords taken both from the ntlk that gensim corpus (the appended ones)\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "stopwords.extend(['know', 'youre', 'want','thats','ad', 'amp', 'id', 'dont','like', 'people','im', 'dont', 'one', 'think', 'would', 'during', 'among', 'thereafter', 'only', 'hers', 'in', 'none', 'with', 'un', 'put', 'hence', 'each', 'would', 'have', 'to', 'itself', 'that', 'seeming', 'hereupon', 'someone', 'eight', 'she', 'forty', 'much', 'throughout', 'less', 'was', 'interest', 'elsewhere', 'already', 'whatever', 'or', 'seem', 'fire', 'however', 'keep', 'detail', 'both', 'yourselves', 'indeed', 'enough', 'too', 'us', 'wherein', 'himself', 'behind', 'everything', 'part', 'made', 'thereupon', 'for', 'nor', 'before', 'front', 'sincere', 'really', 'than', 'alone', 'doing', 'amongst', 'across', 'him', 'another', 'some', 'whoever', 'four', 'other', 'latterly', 'off', 'sometime', 'above', 'often', 'herein', 'am', 'whereby', 'although', 'who', 'should', 'amount', 'anyway', 'else', 'upon', 'this', 'when', 'we', 'few', 'anywhere', 'will', 'though', 'being', 'fill', 'used', 'full', 'thru', 'call', 'whereafter', 'various', 'has', 'same', 'former', 'whereas', 'what', 'had', 'mostly', 'onto', 'go', 'could', 'yourself', 'meanwhile', 'beyond', 'beside', 'ours', 'side', 'our', 'five', 'nobody', 'herself', 'is', 'ever', 'they', 'here', 'eleven', 'fifty', 'therefore', 'nothing', 'not', 'mill', 'without', 'whence', 'get', 'whither', 'then', 'no', 'own', 'many', 'anything', 'etc', 'make', 'from', 'against', 'ltd', 'next', 'afterwards', 'unless', 'while', 'thin', 'beforehand', 'by', 'amoungst', 'you', 'third', 'as', 'those', 'done', 'becoming', 'say', 'either', 'doesn', 'twenty', 'his', 'yet', 'latter', 'somehow', 'are', 'these', 'mine', 'under', 'take', 'whose', 'others', 'over', 'perhaps', 'thence', 'does', 'where', 'two', 'always', 'your', 'wherever', 'became', 'which', 'about', 'but', 'towards', 'still', 'rather', 'quite', 'whether', 'somewhere', 'might', 'do', 'bottom', 'until', 'km', 'yours', 'serious', 'find', 'please', 'hasnt', 'otherwise', 'six', 'toward', 'sometimes', 'of', 'fifteen', 'eg', 'just', 'a', 'me', 'describe', 'why', 'an', 'and', 'may', 'within', 'kg', 'con', 're', 'nevertheless', 'through', 'very', 'anyhow', 'down', 'nowhere', 'now', 'it', 'cant', 'de', 'move', 'hereby', 'how', 'found', 'whom', 'were', 'together', 'again', 'moreover', 'first', 'never', 'below', 'between', 'computer', 'ten', 'into', 'see', 'everywhere', 'there', 'neither', 'every', 'couldnt', 'up', 'several', 'the', 'i', 'becomes', 'don', 'ie', 'been', 'whereupon', 'seemed', 'most', 'noone', 'whole', 'must', 'cannot', 'per', 'my', 'thereby', 'so', 'he', 'name', 'co', 'its', 'everyone', 'if', 'become', 'thick', 'thus', 'regarding', 'didn', 'give', 'all', 'show', 'any', 'using', 'on', 'further', 'around', 'back', 'least', 'since', 'anyone', 'once', 'can', 'bill', 'hereafter', 'be', 'seems', 'their', 'myself', 'nine', 'also', 'system', 'at', 'more', 'out', 'twelve', 'therein', 'almost', 'except', 'last', 'did', 'something', 'besides', 'via', 'whenever', 'formerly', 'cry', 'one', 'hundred', 'sixty', 'after', 'well', 'them', 'namely', 'empty', 'three', 'even', 'along', 'because', 'ourselves', 'such', 'top', 'due', 'inc', 'themselves'])\n",
    "\n",
    "clean_train_data['body']=clean_train_data['body'].apply(lambda x: ' '.join([words for words in x.split() if words not in stopwords]))\n",
    "clean_train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer and stemmer do not seems to make a real difference\n",
    "\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "#from nltk import punkt\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "  \n",
    "#lemmatizer = WordNetLemmatizer() \n",
    "#clean_train_data['body']=clean_train_data['body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "\n",
    "\n",
    "#from nltk.stem import LancasterStemmer\n",
    "#stemmer=LancasterStemmer()\n",
    "\n",
    "#clean_train_data['body']=clean_train_data['body'].apply(lambda x: ' '.join([stemmer.stem(w) for w in nltk.word_tokenize(x)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>wed nearly fanfiction pictures shipping banban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>thanks got heart break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>sure arent confusing cyclops easiest boss mons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>mate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sahil17</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>damn love question heres church fathers earlie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spyfer</td>\n",
       "      <td>NetflixBestOf</td>\n",
       "      <td>sorry post btw lol hope didnt cause guys troub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tehftw</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>right mistaken firearms access weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>timsstuff</td>\n",
       "      <td>movies</td>\n",
       "      <td>thought colm meaney hes role colm meaney john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bullseye4hire</td>\n",
       "      <td>hockey</td>\n",
       "      <td>fack looks mins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit  \\\n",
       "0    Shamus_Aran       mylittlepony   \n",
       "1       Riddance                sex   \n",
       "2  Secret_Wizard       DragonsDogma   \n",
       "3   Penultimatum  malefashionadvice   \n",
       "4      7-SE7EN-7      todayilearned   \n",
       "5        Sahil17       Christianity   \n",
       "6         spyfer      NetflixBestOf   \n",
       "7         tehftw          AskReddit   \n",
       "8      timsstuff             movies   \n",
       "9  Bullseye4hire             hockey   \n",
       "\n",
       "                                                body  \n",
       "0  wed nearly fanfiction pictures shipping banban...  \n",
       "1                             thanks got heart break  \n",
       "2  sure arent confusing cyclops easiest boss mons...  \n",
       "3                                                bro  \n",
       "4                                               mate  \n",
       "5  damn love question heres church fathers earlie...  \n",
       "6  sorry post btw lol hope didnt cause guys troub...  \n",
       "7             right mistaken firearms access weapons  \n",
       "8  thought colm meaney hes role colm meaney john ...  \n",
       "9                                    fack looks mins  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"/home/ale/Scrivania/reddit_chal/train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mylittlepony            0\n",
      "sex                     1\n",
      "DragonsDogma            2\n",
      "malefashionadvice       3\n",
      "todayilearned           4\n",
      "                     ... \n",
      "palegirls            3463\n",
      "onions               3464\n",
      "mumfordandsons       3465\n",
      "infertility          3466\n",
      "HangoutFest          3467\n",
      "Length: 3468, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Extract the subreddit list and mapping it to integers\n",
    "subreddits = clean_train_data.subreddit.unique()\n",
    "subreddits_map = pd.Series(index=subreddits, data=arange(subreddits.shape[0]))\n",
    "\n",
    "print(subreddits_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(group):\n",
    "    group_subreddits = group['subreddit'].values \n",
    "    #print(group_subreddits.type)\n",
    "    idxs = subreddits_map[group_subreddits].values \n",
    "    v = sparse.dok_matrix((1, subreddits.shape[0]))\n",
    "  \n",
    "    \n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] = 1\n",
    "    \n",
    " \n",
    "    return v.tocsr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "   \n",
    "    features_dict[author] = extract_features(group)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x3468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49152 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sparse.vstack([features_dict[author] for author in target.author])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = target.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(group):\n",
    "    group_text = group['body'].values\n",
    "    return \" \".join(group_text)\n",
    "\n",
    "def extract_subrteddit_text(group):\n",
    "    group_text = group['subreddit'].values\n",
    "    return \" \".join(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "    text_dict[author] = extract_text(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_text = [text_dict[author] for author in target.author]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_dict ={}\n",
    "author_dict = {}\n",
    "\n",
    "for subreddit, group in clean_train_data.groupby('subreddit'):\n",
    "    subreddit_dict[subreddit] = extract_text(group)\n",
    "\n",
    "    \n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "    author_dict[author] = extract_text(group)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172269 176730\n"
     ]
    }
   ],
   "source": [
    "#Creating list of words taken from the corpus of top subreddit for men and women\n",
    "#sources: https://www.techlazy.com/best-subreddits-for-men/\n",
    "#http://redditlist.com/\n",
    "\n",
    "\n",
    "men_list=[]\n",
    "men_list.extend(subreddit_dict['malefashionadvice'].split(' '))\n",
    "men_list.extend(subreddit_dict['MensRights'].split(' '))\n",
    "men_list.extend(subreddit_dict['malehairadvice'].split(' '))\n",
    "men_list.extend(subreddit_dict['beards'].split(' '))\n",
    "men_list.extend(subreddit_dict['AskMen'].split(' '))\n",
    "men_list.extend(subreddit_dict['AskMenOver30'].split(' '))\n",
    "\n",
    "\n",
    "women_list=[]\n",
    "women_list.extend(subreddit_dict['AskWomen'].split(' '))\n",
    "women_list.extend(subreddit_dict['Feminism'].split(' '))\n",
    "women_list.extend(subreddit_dict['TheGirlSurvivalGuide'].split(' '))\n",
    "women_list.extend(subreddit_dict['GirlGamers'].split(' '))\n",
    "women_list.extend(subreddit_dict['BodyAcceptance'].split(' '))\n",
    "women_list.extend(subreddit_dict['AskWomenOver30'].split(' '))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(women_list), len(men_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to sort the wordlists by number of appearences and eliminate duplicates\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def sort_unique(lst, n):\n",
    "    result = (item for items, c in Counter(lst).most_common() for item in [items] * c)\n",
    "    unique_list = list(dict.fromkeys(result))\n",
    "    if len (lst)> n:\n",
    "            unique_list = unique_list[:n]\n",
    "    else: \n",
    "            unique_list=unique_list[:len(lst)]\n",
    "    return unique_list\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time              0\n",
      "women             1\n",
      "good              2\n",
      "way               3\n",
      "ive               4\n",
      "               ... \n",
      "prospects      4995\n",
      "carrying       4996\n",
      "furthermore    4997\n",
      "damages        4998\n",
      "approve        4999\n",
      "Length: 5000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "men_sorted_unique=  np.array(sort_unique(subreddit_dict['AskMen'].split(' '), 5000))\n",
    "women_sorted_unique= np.array(sort_unique(subreddit_dict['AskWomen'].split(' '), 5000))\n",
    "\n",
    "M_subreddit_gender_map = pd.Series(index=men_sorted_unique, data=arange(men_sorted_unique.shape[0]))\n",
    "F_subreddit_gender_map = pd.Series(index=women_sorted_unique, data=arange(women_sorted_unique.shape[0]))\n",
    "\n",
    "\n",
    "print(M_subreddit_gender_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create sparse matrices relatives to the m/f features\n",
    "\n",
    "def extract_Mgender_features(group):\n",
    "\n",
    "    #group_subreddits = group['subreddit'].values #raggruppa per subreddits (str) \n",
    "    group_text = group['body'].values\n",
    "                        \n",
    "    idxs = M_subreddit_gender_map[group_text].values\n",
    "\n",
    "    \n",
    "    #print(idxs)\n",
    "    v = sparse.dok_matrix((1, men_sorted_unique.shape[0]))\n",
    "    #print(v)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] =1\n",
    "        \n",
    "    \n",
    "    \n",
    "    return v.tocsr()\n",
    "\n",
    "\n",
    "def extract_Fgender_features(group):\n",
    "\n",
    "    #group_subreddits = group['subreddit'].values #raggruppa per subreddits (str) \n",
    "    group_text = group['body'].values\n",
    "                        \n",
    "    idxs = F_subreddit_gender_map[group_text].values\n",
    "    #print(idxs)\n",
    "    \n",
    "    #print(idxs)\n",
    "    v = sparse.dok_matrix((1, women_sorted_unique.shape[0]))\n",
    "    #print(v)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] +=1\n",
    "        \n",
    "    \n",
    "    \n",
    "    return v.tocsr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the m/dictionaries\n",
    "\n",
    "M_features_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "   \n",
    "    M_features_dict[author] = extract_Mgender_features(group)\n",
    "    \n",
    "    \n",
    "F_features_dict = {}\n",
    "\n",
    "for author, group in clean_train_data.groupby('author'):\n",
    "   \n",
    "    F_features_dict[author] = extract_Fgender_features(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sparse matrices for m/f contributions\n",
    "Xf = sparse.vstack([F_features_dict[author] for author in target.author])\n",
    "Xm = sparse.vstack([M_features_dict[author] for author in target.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x13468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 73879 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stacking all the sparse matrices\n",
    "X=sparse.hstack([X, Xm, Xf]).tocsr()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score ,f1_score,roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form now on i'll try some different classifiers and choose the most performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#\n",
    "\n",
    "clf_lr = LogisticRegression(C=5, max_iter=100)\n",
    "clf_lr.fit(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       350\n",
      "           1       0.77      0.55      0.64       150\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.80      0.74      0.76       500\n",
      "weighted avg       0.81      0.82      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[326,  24],\n",
       "       [ 68,  82]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68317247167436"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
    "kfold_lr = KFold(n_splits=10)\n",
    "result_lr = cross_val_predict(estimator=clf_lr,X=X,y=y,cv=kfold_lr)\n",
    "\n",
    "\n",
    "f1_lr=f1_score(y, result_lr)\n",
    "\n",
    "f1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.849"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lr = accuracy_score(y, result_lr)\n",
    "acc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_lr = roc_auc_score(y, y_score=result_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Model\n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(kernel='linear', probability=True)\n",
    "clf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc = clf_svc.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       350\n",
      "           1       0.82      0.51      0.63       150\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.82      0.73      0.75       500\n",
      "weighted avg       0.82      0.82      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[333,  17],\n",
       "       [ 74,  76]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_svc = KFold(n_splits=10)\n",
    "result_svc = cross_val_predict(estimator=clf_svc,X=X,y=y,cv=kfold_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.663002114164905"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_svc=f1_score(y, result_svc)\n",
    "f1_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_svc = accuracy_score(y, result_svc)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7588135017488633"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score_svc = roc_auc_score(y, y_score=result_svc)\n",
    "roc_auc_score_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.828 0.858 0.828 0.845 0.843]\n",
      "0.8404\n"
     ]
    }
   ],
   "source": [
    "cross_val_svc = cross_val_score(SVC(kernel='linear', probability=True),X,y, cv=5, scoring='accuracy')\n",
    "print (cross_val_svc)\n",
    "print (cross_val_svc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter choice: {'max_depth': 50, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf_1 = RandomForestClassifier()\n",
    "Cs_1 = [10, 500]\n",
    "Cs_2 = [None, 50, 400]\n",
    "grid_1 = GridSearchCV(clf_1, param_grid={'n_estimators': Cs_1,'max_depth': Cs_2}, cv=5)\n",
    "grid_1.fit(X_train, y_train)\n",
    "print (\"best parameter choice:\", grid_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=50, n_estimators=500)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=grid_1.best_params_['n_estimators'], max_depth=grid_1.best_params_['max_depth'])\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       350\n",
      "           1       0.92      0.36      0.52       150\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.85      0.67      0.69       500\n",
      "weighted avg       0.82      0.80      0.77       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[345,   5],\n",
       "       [ 96,  54]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5952976488244123"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_rf = KFold(n_splits=5, random_state=42)\n",
    "result_rf = cross_val_predict(estimator=clf_rf,X=X,y=y,cv=kfold_rf)\n",
    "f1_rf=f1_score(y, result_rf)\n",
    "f1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8382"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_rf = accuracy_score(y, result_rf)\n",
    "acc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7130015457243454"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score_rf = roc_auc_score(y, y_score=result_rf)\n",
    "roc_auc_score_rf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiNB=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiNB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MultiNB = MultiNB.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred_MultiNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       350\n",
      "           1       0.86      0.59      0.70       150\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.86      0.78      0.80       500\n",
      "weighted avg       0.85      0.85      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test, y_pred=y_pred_MultiNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[336,  14],\n",
       "       [ 61,  89]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=y_test, y_pred=y_pred_MultiNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_NB = KFold(n_splits=10)\n",
    "result_NB = cross_val_predict(estimator=MultiNB,X=X,y=y,cv=kfold_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_NB = accuracy_score(y, result_NB)\n",
    "acc_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7912442725664486"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score_NB = roc_auc_score(y, y_score=result_NB)\n",
    "roc_auc_score_NB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiNB is the best classifier for this problem\n"
     ]
    }
   ],
   "source": [
    "classifiers = ['Logistic Regression', 'Random Forest Classifier', 'SVC', 'MultiNB']\n",
    "acc = np.array([acc_lr, acc_rf, acc_svc, acc_NB])\n",
    "max_acc = np.argmax(acc)\n",
    "rocauc = np.array([roc_auc_score_lr, roc_auc_score_rf, roc_auc_score_svc, roc_auc_score_NB])\n",
    "max_rocauc = np.argmax(rocauc)\n",
    "print (classifiers[max_acc] + ' is the best classifier for this problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=.01).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.3161325289447928\n",
      "Best Params:  {'alpha': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  28 out of  35 | elapsed:    0.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  35 out of  35 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=4)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Mean:  -0.0013330575744815222\n",
      "STD:  0.001458637859018578\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "# Train model with default alpha=1 and l1_ratio=0.5\n",
    "elastic_net = ElasticNet(alpha=1, l1_ratio=0.5).fit(X_train, y_train)\n",
    "# get cross val scores\n",
    "\n",
    "def get_cv_scores(model):\n",
    "    scores = cross_val_score(model,\n",
    "                             X_train,\n",
    "                             y_train,\n",
    "                             cv=5,\n",
    "                             scoring='r2')\n",
    "    \n",
    "    print('CV Mean: ', np.mean(scores))\n",
    "    print('STD: ', np.std(scores))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "get_cv_scores(elastic_net)\n",
    "\n",
    "\n",
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "l1_ratio = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "param_grid = dict(alpha=alpha, l1_ratio=l1_ratio)\n",
    "grid = GridSearchCV(estimator=elastic_net, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "# pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Create param grid.\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']},\n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "    'classifier__n_estimators' : list(range(10,101,10)),\n",
    "    'classifier__max_features' : list(range(6,32,5))}\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "best_clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
